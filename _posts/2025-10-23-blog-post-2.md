---
title: 'Another model not to be overlooked: Vision GNN'
date: 2025-10-23
permalink: /posts/2013/08/blog-post-2/
tags:
  - visual gnn
  - graph representation
    
---

Another interesting paper [The paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/3743e69c8e47eb2e6d3afaea80e439fb-Paper-Conference.pdf) that won't be overlooked, and it is published in NeurIPS 2022. Quite old, but it still gives an interesting idea about graphs for vision.
Mostly the architecture for image, centered on convolutional or grid style. Or for the sequential part, there is a vision transformer (ViT), which uses a sequence image approach instead. However, these approaches are inflexible for processing an image. Let's say an object is composed of composition parts, for example, the human body can be composed of the head, arms, legs, upper body, and lower body. By analyzing these parts as a graph, we are able to recognize the human. Therefore, viewing this image as a graph instead can be more flexible and effective for visual perception. 

In this paper, the authors build the vision graph neural network (ViG) for visual tasks. They do not treat each pixel as a node, but rather divide the input image to several patches and view each patch as a node. After that, they construct the graph of image patches, then use the ViG model to transform and exchange information among nodes. 
The ViG module itself consists of 2 parts, namely grapher and FFNN (Feed Forward Neural Network). Grapher module is constructed based on graph convolution for graph information processing. On the other hand, FFNN is utilized for node feature transformation. 

<!---
Headings are cool
======

You can have many headings
======
-->
Neural network architecture
------

a. Isotrophic architecture
The main body has features with equal size and shape throughout the network, such as ViT and ResMLP. To enlarge the receptive field gradually, the number of neighboring nodes k increases from 9 to 18 linearly as layers go deeper in the models.
b. Pyramid architecture
Extracting features with gradually smaller spatial size as the layer goes deeper, such as ResNet and PVT. The pyramid architecture is effective for visual tasks.


Visualization
------
To better understand how our ViG model works, below a visualization of the constructed graph. It is quite nice to see that the model can select the content-related nodes as the first-order neighbors. Moving to the shallow layer, the neighbor nodes tend to select based on low-level and local features, namely color and texture. In the deep layer, such as the 12th block, the neighbors of center nodes are more semantically and can be able to detected into the same category. It would be good to understand the semantic representation and recognize the objects. 
